<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Eunsu Baek </title> <meta name="author" content="Eunsu Baek"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://edw2n.github.io/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">about <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title"> <span class="font-weight-bold">Eunsu</span> Baek </h1> <p class="desc"><a href="#">Ph.D. Student</a> in SNU GSDS.</p> </header> <article> <div class="profile float-left"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/IMG_1784-480.webp 480w,/assets/img/IMG_1784-800.webp 800w,/assets/img/IMG_1784-1400.webp 1400w," sizes="(min-width: 930px) 270.0px, (min-width: 576px) 30vw, 95vw" type="image/webp"> <img src="/assets/img/IMG_1784.jpg?427c757ba66792ea6234dbc45a40c95b" class="img-fluid z-depth-1 rounded" width="100%" height="auto" alt="IMG_1784.jpg" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> <div class="more-info"> <p> </p> </div> </div> <div class="clearfix"> <p>Hi, I’m <strong>Eunsu Baek</strong> — a Ph.D. student at the <strong>Graduate School of Data Science</strong>, <em>Seoul National University</em>, advised by <strong>Prof. Hyung-Sin Kim</strong>. I previously earned my Master’s in <strong>HCI</strong> and <strong>visualization</strong> at <em>HCIL Lab</em> under <strong>Prof. Jinuk Seo</strong>.</p> <p>I research <strong>robust</strong>, <strong>reliable</strong>, and <strong>embodied AI</strong>, focusing on: <strong>Adaptive Sensing for DNNs</strong>, <strong>Domain Generalization &amp; Adaptation</strong>, and <strong>OOD (Out-of-Distribution) Detection</strong>.</p> <p>I design sensing-centric methods that boost robustness not just via model training, but by optimizing data acquisition—like how lenses improve human vision.</p> <p>Thanks for visiting — feel free to explore or <a href="mailto:beshu9407@snu.ac.kr">connect with me</a>!</p> </div> <h2> <a href="/news/" style="color: inherit">news</a> </h2> <div class="news"> <div class="table-responsive" style="max-height: 60vw"> <table class="table table-sm table-borderless"> <tr> <th scope="row" style="width: 20%">Jun 24, 2025</th> <td> My paper on Medical AI (<a href="https://openreview.net/pdf?id=He2FGdmsas" rel="external nofollow noopener" target="_blank">ColonOOD</a>) has been accepted for <b><a href="https://iclr.cc/" rel="external nofollow noopener" target="_blank">ESWA (Impact factor 7.5)</a></b>. </td> </tr> <tr> <th scope="row" style="width: 20%">Jan 23, 2025</th> <td> My paper on the first <a href="https://openreview.net/pdf?id=He2FGdmsas" rel="external nofollow noopener" target="_blank">Adaptive Sensing Framework for DNNs</a> has been accepted for <b><a href="https://iclr.cc/" rel="external nofollow noopener" target="_blank">ICLR 2025</a></b>. </td> </tr> <tr> <th scope="row" style="width: 20%">Dec 28, 2024</th> <td> I received the <b><a href="https://gsds.snu.ac.kr/2024%EB%85%84%EB%8F%84-%E3%80%8Cbk21-%EC%9A%B0%EC%88%98%EB%85%BC%EB%AC%B8%EC%83%81%E3%80%8D/" rel="external nofollow noopener" target="_blank">BK21 Outstanding Paper Award 2024</a></b> for <a href="https://openaccess.thecvf.com/content/CVPR2024/html/Baek_Unexplored_Faces_of_Robustness_and_Out-of-Distribution_Covariate_Shifts_in_Environment_CVPR_2024_paper.html" rel="external nofollow noopener" target="_blank">ImageNet-ES (CVPR 2024)</a>. </td> </tr> <tr> <th scope="row" style="width: 20%">Jul 31, 2024</th> <td> My dataset (<a href="https://github.com/Edw2n/ImageNet-ES" rel="external nofollow noopener" target="_blank">ImageNet-ES, CVPR 24</a>) has been integrated into the <b><a href="https://github.com/Jingkang50/OpenOOD" rel="external nofollow noopener" target="_blank">OpenOOD</a></b> benchmark. </td> </tr> <tr> <th scope="row" style="width: 20%">Jun 05, 2024</th> <td> My team won <a href="awards/240605_vto_demo/">People’s Choice Demo Award</a> (<a href="https://www.youtube.com/watch?v=YTExc8W5BzM" rel="external nofollow noopener" target="_blank">Virtual Try-on</a>) at <b><a href="https://www.sigmobile.org/mobisys/2024/" rel="external nofollow noopener" target="_blank">Mobisys 2024</a></b>. </td> </tr> <tr> <th scope="row" style="width: 20%">Feb 27, 2024</th> <td> My paper on <a href="https://arxiv.org/abs/2404.15882" rel="external nofollow noopener" target="_blank">Robustness benchmark</a> has been accepted for <b><a href="https://cvpr.thecvf.com" rel="external nofollow noopener" target="_blank">CVPR 2024</a></b>. </td> </tr> <tr> <th scope="row" style="width: 20%">Oct 30, 2023</th> <td> I took the <b>1st place (out of 212 teams)</b> in <b><a href="https://dacon.io/competitions/official/236132/overview/description" rel="external nofollow noopener" target="_blank">Samsung AI Challenge</a></b> (<a href="awards/241030_ss_ai_challenge/">camera-invariant domain adaptation</a>). </td> </tr> <tr> <th scope="row" style="width: 20%">Oct 19, 2023</th> <td> My paper on <a href="https://dl.acm.org/doi/abs/10.1145/3631420" rel="external nofollow noopener" target="_blank">Generative/Edge/Fashion AI </a> has been accepted for <b><a href="https://dl.acm.org/journal/imwut" rel="external nofollow noopener" target="_blank">IMWUT</a></b> (and <b><a href="https://www.ubicomp.org/ubicomp-iswc-2024/" rel="external nofollow noopener" target="_blank">UbiComp 2024</a></b>) </td> </tr> <tr> <th scope="row" style="width: 20%">Oct 31, 2022</th> <td> I took the <b>1st place </b> in <b><a href="https://www.youtube.com/watch?v=sdZUG7mFNM0&amp;t=6s" rel="external nofollow noopener" target="_blank"> Google Coral-board Challenge</a></b> <a href="awards/231031_google_coralboard/">(Navible: Navigation for blinds) </a>. </td> </tr> <tr> <th scope="row" style="width: 20%">Jul 01, 2022</th> <td> I have joined in AIoT Lab. </td> </tr> </table> </div> </div> <h2> <a href="/publications/" style="color: inherit">selected publications</a> </h2> <div class="publications"> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#1976D2"> <a href="https://iclr.cc/" rel="external nofollow noopener" target="_blank">ICLR</a> </abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/lens-thumb-short-480.webp 480w,/assets/img/publication_preview/lens-thumb-short-800.webp 800w,/assets/img/publication_preview/lens-thumb-short-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/lens-thumb-short.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="lens-thumb-short.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="lens" class="col-sm-8"> <div class="title">Adaptive Camera Sensor for Vision Model</div> <div class="author"> <em>Eunsu Baek</em>, Taesik Gong, and Hyung‑Sin Kim </div> <div class="periodical"> <em>In The 13th International Conference on Learning Representations</em> , Apr 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://openreview.net/pdf?id=He2FGdmsas" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/Edw2n/Lens" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Domain shift remains a persistent challenge in deep-learning-based computer vision, often requiring extensive model modifications or large labeled datasets to address. Inspired by human visual perception, which adjusts input quality through corrective lenses rather than over-training the brain, we propose Lens, a novel camera sensor control method that enhances model performance by capturing high-quality images from the model’s perspective rather than relying on traditional human-centric sensor control. Lens is lightweight and adapts sensor parameters to specific models and scenes in real-time. At its core, Lens utilizes VisiT, a training-free, model-specific quality indicator that evaluates individual unlabeled samples at test time using confidence scores without additional adaptation costs. To validate Lens, we introduce ImageNet-ES Diverse, a new benchmark dataset capturing natural perturbations from varying sensor and lighting conditions. Extensive experiments on both ImageNet-ES and our new ImageNet-ES Diverse show that Lens significantly improves model accuracy across various baseline schemes for sensor control and model modification while maintaining low latency in image captures. Lens effectively compensates for large model size differences and integrates synergistically with model improvement techniques. Our code and dataset are available at github.com/Edw2n/Lens.git.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">lens</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Baek, Eunsu and Gong, Taesik and Kim, Hyung‑Sin}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Adaptive Camera Sensor for Vision Model}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{The 13th International Conference on Learning Representations}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">apr</span><span class="p">,</span>
  <span class="na">tags</span> <span class="p">=</span> <span class="s">{s4d}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#009f36"> <a href="https://cvpr.thecvf.com" rel="external nofollow noopener" target="_blank">CVPR</a> </abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/ImageNet-ES-480.webp 480w,/assets/img/publication_preview/ImageNet-ES-800.webp 800w,/assets/img/publication_preview/ImageNet-ES-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/ImageNet-ES.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="ImageNet-ES.jpg" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="imagenet-es" class="col-sm-8"> <div class="title">Unexplored Faces of Robustness and Out-of-Distribution: Covariate Shifts in Environment and Sensor Domains</div> <div class="author"> <em>Eunsu Baek</em>, Keondo Park, Jiyoon Kim, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Hyung-Sin Kim' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em> , Jun 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://openaccess.thecvf.com/content/CVPR2024/html/Baek_Unexplored_Faces_of_Robustness_and_Out-of-Distribution_Covariate_Shifts_in_Environment_CVPR_2024_paper.html" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://youtu.be/rdfUkYSWpik?feature=shared" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://github.com/Edw2n/ImageNet-ES" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Computer vision applications predict on digital images acquired by a camera from physical scenes through light. However, conventional robustness benchmarks rely on perturbations in digitized images, diverging from distribution shifts occurring in the image acquisition process. To bridge this gap, we introduce a new distribution shift dataset, ImageNet-ES, comprising variations in environmental and camera sensor factors by directly capturing 202k images with a real camera in a controllable testbed. With the new dataset, we evaluate out-of-distribution (OOD) detection and model robustness. We find that existing OOD detection methods do not cope with the covariate shifts in ImageNet-ES, implying that the definition and detection of OOD should be revisited to embrace real-world distribution shifts. We also observe that the model becomes more robust in both ImageNet-C and -ES by learning environment and sensor variations in addition to existing digital augmentations. Lastly, our results suggest that effective shift mitigation via camera sensor control can significantly improve performance without increasing model size. With these findings, our benchmark may aid future research on robustness, OOD, and camera sensor control for computer vision. Our code and dataset are available at this https://github.com/Edw2n/ImageNet-ES</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">imagenet-es</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Baek, Eunsu and Park, Keondo and Kim, Jiyoon and Kim, Hyung-Sin}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Unexplored Faces of Robustness and Out-of-Distribution: Covariate Shifts in Environment and Sensor Domains}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jun</span><span class="p">,</span>
  <span class="na">tags</span> <span class="p">=</span> <span class="s">{s4d}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100"> <a href="https://www.ubicomp.org/ubicomp-iswc-2024/" rel="external nofollow noopener" target="_blank">IMWUT/UbiComp</a> </abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/MIRROR_Virtual_TryOn-480.webp 480w,/assets/img/publication_preview/MIRROR_Virtual_TryOn-800.webp 800w,/assets/img/publication_preview/MIRROR_Virtual_TryOn-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/MIRROR_Virtual_TryOn.gif" class="preview z-depth-1 rounded" width="100%" height="auto" alt="MIRROR_Virtual_TryOn.gif" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="10.1145/3631420" class="col-sm-8"> <div class="title">MIRROR: Towards Generalizable On-Device Video Virtual Try-On for Mobile Shopping</div> <div class="author"> Dong-Sig Kang, <em>Eunsu Baek</em>, Sungwook Son, and <span class="more-authors" title="click to view 3 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '3 more authors' ? 'Youngki Lee, Taesik Gong, Hyung-Sin Kim' : '3 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">3 more authors</span> </div> <div class="periodical"> <em>Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies (also published in ACM UbiComp, Oct 2024)</em>, Dec 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://dl.acm.org/doi/abs/10.1145/3631420" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://dl.acm.org/doi/pdf/10.1145/3631420" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://youtu.be/wGtxRnmfNIw?feature=sharedhttps://youtu.be/wGtxRnmfNIw?feature=shared" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://github.com/Ds-Kang/MIRROR.git" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>We present MIRROR, an on-device video virtual try-on (VTO) system that provides realistic, private, and rapid experiences in mobile clothes shopping. Despite recent advancements in generative adversarial networks (GANs) for VTO, designing MIRROR involves two challenges: (1) data discrepancy due to restricted training data that miss various poses, body sizes, and backgrounds and (2) local computation overhead that uses up 24% of battery for converting only a single video. To alleviate the problems, we propose a generalizable VTO GAN that not only discerns intricate human body semantics but also captures domain-invariant features without requiring additional training data. In addition, we craft lightweight, reliable clothes/pose-tracking that generates refined pixel-wise warping flow without neural-net computation. As a holistic system, MIRROR integrates the new VTO GAN and tracking method with meticulous pre/post-processing, operating in two distinct phases (on/offline). Our results on Android smartphones and real-world user videos show that compared to a cutting-edge VTO GAN, MIRROR achieves 6.5x better accuracy with 20.1x faster video conversion and 16.9x less energy consumption.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">10.1145/3631420</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Kang, Dong-Sig and Baek, Eunsu and Son, Sungwook and Lee, Youngki and Gong, Taesik and Kim, Hyung-Sin}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{MIRROR: Towards Generalizable On-Device Video Virtual Try-On for Mobile Shopping}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">issue_date</span> <span class="p">=</span> <span class="s">{December 2023}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computing Machinery}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{New York, NY, USA}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{7}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{4}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1145/3631420}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1145/3631420}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies (also published in ACM UbiComp, Oct 2024)}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">dec</span><span class="p">,</span>
  <span class="na">articleno</span> <span class="p">=</span> <span class="s">{163}</span><span class="p">,</span>
  <span class="na">numpages</span> <span class="p">=</span> <span class="s">{27}</span><span class="p">,</span>
  <span class="na">tags</span> <span class="p">=</span> <span class="s">{mirror}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> </div> <div class="social"> <div class="contact-icons"> <a href="mailto:%62%65%73%68%75%39%34%30%37@%73%6E%75.%61%63.%6B%72" title="email"><i class="fa-solid fa-envelope"></i></a> <a href="https://scholar.google.com/citations?user=VihDCUsAAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> <a href="https://github.com/edw2n" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-github"></i></a> <a href="https://www.linkedin.com/in/eunsu-baek-4449282a9" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-linkedin"></i></a> </div> <div class="contact-note"></div> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Eunsu Baek. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?b7816bd189846d29eded8745f9c4cf77"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.min.js" integrity="sha256-rjmgmaB99riUNcdlrDtcAiwtLIojSxNyUFdl+Qh+rB4=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?601a2d3465e2a52bec38b600518d5f70"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let theme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===theme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"about",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-cv",title:"cv",description:"",section:"Navigation",handler:()=>{window.location.href="/cv/"}},{id:"nav-publications",title:"publications",description:"Publications about some works that I have done or collaborated with. You can download the documents to read them in full.",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"nav-projects",title:"projects",description:"Projects about some works that I have done or collaborated with.",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"nav-teaching",title:"teaching",description:"Materials for courses you taught. Replace this text with your description.",section:"Navigation",handler:()=>{window.location.href="/teaching/"}},{id:"post-google-gemini-updates-flash-1-5-gemma-2-and-project-astra",title:"Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra",description:"We\u2019re sharing updates across our Gemini family of models and a glimpse of Project Astra, our vision for the future of AI assistants.",section:"Posts",handler:()=>{window.open("https://blog.google/technology/ai/google-gemini-update-flash-ai-assistant-io-2024/","_blank")}},{id:"post-a-post-with-tabs",title:"a post with tabs",description:"this is what included tabs in a post could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/tabs/"}},{id:"post-a-post-with-typograms",title:"a post with typograms",description:"this is what included typograms code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/typograms/"}},{id:"post-a-post-that-can-be-cited",title:"a post that can be cited",description:"this is what a post that can be cited looks like",section:"Posts",handler:()=>{window.location.href="/blog/2024/post-citation/"}},{id:"post-a-post-with-pseudo-code",title:"a post with pseudo code",description:"this is what included pseudo code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/pseudocode/"}},{id:"post-a-post-with-code-diff",title:"a post with code diff",description:"this is how you can display code diffs",section:"Posts",handler:()=>{window.location.href="/blog/2024/code-diff/"}},{id:"post-a-post-with-advanced-image-components",title:"a post with advanced image components",description:"this is what advanced image components could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/advanced-images/"}},{id:"post-a-post-with-vega-lite",title:"a post with vega lite",description:"this is what included vega lite code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/vega-lite/"}},{id:"post-a-post-with-geojson",title:"a post with geojson",description:"this is what included geojson code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/geojson-map/"}},{id:"post-a-post-with-echarts",title:"a post with echarts",description:"this is what included echarts code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/echarts/"}},{id:"post-a-post-with-chart-js",title:"a post with chart.js",description:"this is what included chart.js code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/chartjs/"}},{id:"post-a-post-with-tikzjax",title:"a post with TikZJax",description:"this is what included TikZ code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2023/tikzjax/"}},{id:"post-a-post-with-bibliography",title:"a post with bibliography",description:"an example of a blog post with bibliography",section:"Posts",handler:()=>{window.location.href="/blog/2023/post-bibliography/"}},{id:"post-a-post-with-jupyter-notebook",title:"a post with jupyter notebook",description:"an example of a blog post with jupyter notebook",section:"Posts",handler:()=>{window.location.href="/blog/2023/jupyter-notebook/"}},{id:"post-a-post-with-custom-blockquotes",title:"a post with custom blockquotes",description:"an example of a blog post with custom blockquotes",section:"Posts",handler:()=>{window.location.href="/blog/2023/custom-blockquotes/"}},{id:"post-a-post-with-table-of-contents-on-a-sidebar",title:"a post with table of contents on a sidebar",description:"an example of a blog post with table of contents on a sidebar",section:"Posts",handler:()=>{window.location.href="/blog/2023/sidebar-table-of-contents/"}},{id:"post-a-post-with-audios",title:"a post with audios",description:"this is what included audios could look like",section:"Posts",handler:()=>{window.location.href="/blog/2023/audios/"}},{id:"post-a-post-with-videos",title:"a post with videos",description:"this is what included videos could look like",section:"Posts",handler:()=>{window.location.href="/blog/2023/videos/"}},{id:"post-displaying-beautiful-tables-with-bootstrap-tables",title:"displaying beautiful tables with Bootstrap Tables",description:"an example of how to use Bootstrap Tables",section:"Posts",handler:()=>{window.location.href="/blog/2023/tables/"}},{id:"post-a-post-with-table-of-contents",title:"a post with table of contents",description:"an example of a blog post with table of contents",section:"Posts",handler:()=>{window.location.href="/blog/2023/table-of-contents/"}},{id:"post-a-post-with-giscus-comments",title:"a post with giscus comments",description:"an example of a blog post with giscus comments",section:"Posts",handler:()=>{window.location.href="/blog/2022/giscus-comments/"}},{id:"post-displaying-external-posts-on-your-al-folio-blog",title:"Displaying External Posts on Your al-folio Blog",description:"",section:"Posts",handler:()=>{window.open("https://medium.com/@al-folio/displaying-external-posts-on-your-al-folio-blog-b60a1d241a0a?source=rss-17feae71c3c4------2","_blank")}},{id:"post-a-post-with-redirect",title:"a post with redirect",description:"you can also redirect to assets like pdf",section:"Posts",handler:()=>{window.location.href="/assets/pdf/example_pdf.pdf"}},{id:"post-a-post-with-diagrams",title:"a post with diagrams",description:"an example of a blog post with diagrams",section:"Posts",handler:()=>{window.location.href="/blog/2021/diagrams/"}},{id:"post-a-distill-style-blog-post",title:"a distill-style blog post",description:"an example of a distill-style blog post and main elements",section:"Posts",handler:()=>{window.location.href="/blog/2021/distill/"}},{id:"post-a-post-with-github-metadata",title:"a post with github metadata",description:"a quick run down on accessing github metadata.",section:"Posts",handler:()=>{window.location.href="/blog/2020/github-metadata/"}},{id:"post-a-post-with-twitter",title:"a post with twitter",description:"an example of a blog post with twitter",section:"Posts",handler:()=>{window.location.href="/blog/2020/twitter/"}},{id:"post-a-post-with-disqus-comments",title:"a post with disqus comments",description:"an example of a blog post with disqus comments",section:"Posts",handler:()=>{window.location.href="/blog/2015/disqus-comments/"}},{id:"post-a-post-with-math",title:"a post with math",description:"an example of a blog post with some math",section:"Posts",handler:()=>{window.location.href="/blog/2015/math/"}},{id:"post-a-post-with-code",title:"a post with code",description:"an example of a blog post with some code",section:"Posts",handler:()=>{window.location.href="/blog/2015/code/"}},{id:"post-a-post-with-images",title:"a post with images",description:"this is what included images could look like",section:"Posts",handler:()=>{window.location.href="/blog/2015/images/"}},{id:"post-a-post-with-formatting-and-links",title:"a post with formatting and links",description:"march &amp; april, looking forward to summer",section:"Posts",handler:()=>{window.location.href="/blog/2015/formatting-and-links/"}},{id:"news-my-paper-on-medical-ai-lt-a-href-quot-https-openreview-net-pdf-id-he2fgdmsas-quot-gt-colonood-lt-a-gt-has-been-accepted-for-lt-b-gt-lt-a-href-quot-https-iclr-cc-quot-gt-eswa-impact-factor-7-5-lt-a-gt-lt-b-gt",title:"My paper on Medical AI (&lt;a href=&quot;https://openreview.net/pdf?id=He2FGdmsas&quot;&gt;ColonOOD&lt;/a&gt;) has been accepted for &lt;b&gt;&lt;a href=&quot;https://iclr.cc/&quot;&gt;ESWA (Impact factor 7.5)&lt;/a&gt;&lt;/b&gt;.",description:"",section:"News"},{id:"news-my-paper-on-the-first-lt-a-href-quot-https-openreview-net-pdf-id-he2fgdmsas-quot-gt-adaptive-sensing-framework-for-dnns-lt-a-gt-has-been-accepted-for-lt-b-gt-lt-a-href-quot-https-iclr-cc-quot-gt-iclr-2025-lt-a-gt-lt-b-gt",title:"My paper on the first &lt;a href=&quot;https://openreview.net/pdf?id=He2FGdmsas&quot;&gt;Adaptive Sensing Framework for DNNs&lt;/a&gt; has been accepted for &lt;b&gt;&lt;a href=&quot;https://iclr.cc/&quot;&gt;ICLR 2025&lt;/a&gt;&lt;/b&gt;.",description:"",section:"News"},{id:"news-i-received-the-lt-b-gt-lt-a-href-quot-https-gsds-snu-ac-kr-2024\ub144\ub3c4-bk21-\uc6b0\uc218\ub17c\ubb38\uc0c1-quot-gt-bk21-outstanding-paper-award-2024-lt-a-gt-lt-b-gt-for-lt-a-href-quot-https-openaccess-thecvf-com-content-cvpr2024-html-baek-unexplored-faces-of-robustness-and-out-of-distribution-covariate-shifts-in-environment-cvpr-2024-paper-html-quot-gt-imagenet-es-cvpr-2024-lt-a-gt",title:"I received the &lt;b&gt;&lt;a href=&quot;https://gsds.snu.ac.kr/2024\ub144\ub3c4-\u300cbk21-\uc6b0\uc218\ub17c\ubb38\uc0c1\u300d/&quot;&gt;BK21 Outstanding Paper Award 2024&lt;/a&gt;&lt;/b&gt; for &lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2024/html/Baek_Unexplored_Faces_of_Robustness_and_Out-of-Distribution_Covariate_Shifts_in_Environment_CVPR_2024_paper.html&quot;&gt;ImageNet-ES (CVPR 2024)&lt;/a&gt;.",description:"",section:"News"},{id:"news-my-dataset-lt-a-href-quot-https-github-com-edw2n-imagenet-es-quot-gt-imagenet-es-cvpr-24-lt-a-gt-has-been-integrated-into-the-lt-b-gt-lt-a-href-quot-https-github-com-jingkang50-openood-quot-gt-openood-lt-a-gt-lt-b-gt-benchmark",title:"My dataset (&lt;a href=&quot;https://github.com/Edw2n/ImageNet-ES&quot;&gt;ImageNet-ES, CVPR 24&lt;/a&gt;) has been integrated into the &lt;b&gt;&lt;a href=&quot;https://github.com/Jingkang50/OpenOOD&quot;&gt;OpenOOD&lt;/a&gt;&lt;/b&gt; benchmark.",description:"",section:"News"},{id:"news-my-team-won-lt-a-href-quot-awards-240605-vto-demo-quot-gt-people-s-choice-demo-award-lt-a-gt-lt-a-href-quot-https-www-youtube-com-watch-v-ytexc8w5bzm-quot-gt-virtual-try-on-lt-a-gt-at-lt-b-gt-lt-a-href-quot-https-www-sigmobile-org-mobisys-2024-quot-gt-mobisys-2024-lt-a-gt-lt-b-gt",title:"My team won &lt;a href=&quot;awards/240605_vto_demo/&quot;&gt;People\u2019s Choice Demo Award&lt;/a&gt; (&lt;a href=&quot;https://www.youtube.com/watch?v=YTExc8W5BzM&quot;&gt;Virtual Try-on&lt;/a&gt;) at &lt;b&gt;&lt;a href=&quot;https://www.sigmobile.org/mobisys/2024/&quot;&gt;Mobisys 2024&lt;/a&gt;&lt;/b&gt;.",description:"",section:"News"},{id:"news-my-paper-on-lt-a-href-quot-https-arxiv-org-abs-2404-15882-quot-gt-robustness-benchmark-lt-a-gt-has-been-accepted-for-lt-b-gt-lt-a-href-quot-https-cvpr-thecvf-com-quot-gt-cvpr-2024-lt-a-gt-lt-b-gt",title:"My paper on &lt;a href=&quot;https://arxiv.org/abs/2404.15882&quot;&gt;Robustness benchmark&lt;/a&gt; has been accepted for &lt;b&gt;&lt;a href=&quot;https://cvpr.thecvf.com&quot;&gt;CVPR 2024&lt;/a&gt;&lt;/b&gt;.",description:"",section:"News"},{id:"news-i-took-the-lt-b-gt-1st-place-out-of-212-teams-lt-b-gt-in-lt-b-gt-lt-a-href-quot-https-dacon-io-competitions-official-236132-overview-description-quot-gt-samsung-ai-challenge-lt-a-gt-lt-b-gt-lt-a-href-quot-awards-241030-ss-ai-challenge-quot-gt-camera-invariant-domain-adaptation-lt-a-gt",title:"I took the &lt;b&gt;1st place (out of 212 teams)&lt;/b&gt; in &lt;b&gt;&lt;a href=&quot;https://dacon.io/competitions/official/236132/overview/description&quot;&gt;Samsung AI Challenge&lt;/a&gt;&lt;/b&gt; (&lt;a href=&quot;awards/241030_ss_ai_challenge/&quot;&gt;camera-invariant domain adaptation&lt;/a&gt;).",description:"",section:"News"},{id:"news-my-paper-on-lt-a-href-quot-https-dl-acm-org-doi-abs-10-1145-3631420-quot-gt-generative-edge-fashion-ai-lt-a-gt-has-been-accepted-for-lt-b-gt-lt-a-href-quot-https-dl-acm-org-journal-imwut-quot-gt-imwut-lt-a-gt-lt-b-gt-and-lt-b-gt-lt-a-href-quot-https-www-ubicomp-org-ubicomp-iswc-2024-quot-gt-ubicomp-2024-lt-a-gt-lt-b-gt",title:"My paper on &lt;a href=&quot;https://dl.acm.org/doi/abs/10.1145/3631420&quot;&gt;Generative/Edge/Fashion AI &lt;/a&gt; has been accepted for &lt;b&gt;&lt;a href=&quot;https://dl.acm.org/journal/imwut&quot;&gt;IMWUT&lt;/a&gt;&lt;/b&gt; (and &lt;b&gt;&lt;a href=&quot;https://www.ubicomp.org/ubicomp-iswc-2024/&quot;&gt;UbiComp 2024&lt;/a&gt;&lt;/b&gt;)",description:"",section:"News"},{id:"news-i-took-the-lt-b-gt-1st-place-lt-b-gt-in-lt-b-gt-lt-a-href-quot-https-www-youtube-com-watch-v-sdzug7mfnm0-amp-amp-t-6s-quot-gt-google-coral-board-challenge-lt-a-gt-lt-b-gt-lt-a-href-quot-awards-231031-google-coralboard-quot-gt-navible-navigation-for-blinds-lt-a-gt",title:"I took the &lt;b&gt;1st place &lt;/b&gt; in &lt;b&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=sdZUG7mFNM0&amp;amp;t=6s&quot;&gt; Google Coral-board Challenge&lt;/a&gt;&lt;/b&gt; &lt;a href=&quot;awards/231031_google_coralboard/&quot;&gt;(Navible: Navigation for blinds) &lt;/a&gt;.",description:"",section:"News"},{id:"news-i-have-joined-in-aiot-lab",title:"I have joined in AIoT Lab.",description:"",section:"News"},{id:"awards-lt-b-gt-navible-amp-58-navigation-for-blinds-1st-place-lt-b-gt-in-lt-a-href-quot-https-www-youtube-com-watch-v-sdzug7mfnm0-amp-t-6s-quot-gt-google-coral-board-challenge-lt-a-gt",title:"&lt;b&gt;[Navible&amp;#58 Navigation for Blinds] 1st place&lt;/b&gt; in &lt;a href=&quot;https://www.youtube.com/watch?v=sdZUG7mFNM0&amp;t=6s&quot;&gt;Google Coral-board Challenge&lt;/a&gt;.",description:"My team (Jupyo, Sehyun, Dongik and me) took 1st place in Google Coral-baord Competition 2022.\ud83d\ude04",section:"Awards",handler:()=>{window.location.href="/awards/231031_google_coralboard/"}},{id:"awards-lt-b-gt-people-s-choice-demo-runner-up-award-lt-b-gt-at-lt-b-gt-lt-a-href-quot-https-www-sigmobile-org-mobisys-2024-quot-gt-mobisys-2024-lt-a-gt-lt-b-gt",title:"&lt;b&gt;People\u2019s Choice Demo Runner-up Award&lt;b&gt; at &lt;b&gt;&lt;a href=&quot;https://www.sigmobile.org/mobisys/2024/&quot;&gt;Mobisys 2024&lt;/a&gt;&lt;/b&gt;.",description:"At MobiSys 2024, my team won the People&#39;s Choice Demo Runner-up Award for our Virtual Try-on. \ud83d\ude04",section:"Awards",handler:()=>{window.location.href="/awards/240605_vto_demo/"}},{id:"awards-lt-b-gt-1st-place-lt-b-gt-in-lt-a-href-quot-https-dacon-io-competitions-official-236132-overview-description-quot-gt-2023-samsung-ai-challenge-lt-a-gt",title:"&lt;b&gt;1st place&lt;/b&gt; in &lt;a href=&quot;https://dacon.io/competitions/official/236132/overview/description&quot;&gt;2023 Samsung AI Challenge&lt;/a&gt;.",description:"My team (Keondo and me) took 1st place (out of 212 teams) in Samsung AI Challenge (camera-invariant domain adaptation). \ud83d\ude04",section:"Awards",handler:()=>{window.location.href="/awards/241030_ss_ai_challenge/"}},{id:"awards-project-3-with-very-long-name",title:"project 3 with very long name",description:"a project that redirects to another website",section:"Awards",handler:()=>{window.location.href="/awards/3_project/"}},{id:"awards-project-4",title:"project 4",description:"another without an image",section:"Awards",handler:()=>{window.location.href="/awards/4_project/"}},{id:"awards-project-5",title:"project 5",description:"a project with a background image",section:"Awards",handler:()=>{window.location.href="/awards/5_project/"}},{id:"awards-project-6",title:"project 6",description:"a project with no image",section:"Awards",handler:()=>{window.location.href="/awards/6_project/"}},{id:"awards-project-7",title:"project 7",description:"with background image",section:"Awards",handler:()=>{window.location.href="/awards/7_project/"}},{id:"awards-project-8",title:"project 8",description:"an other project with a background image and giscus comments",section:"Awards",handler:()=>{window.location.href="/awards/8_project/"}},{id:"awards-project-9",title:"project 9",description:"another project with an image \ud83c\udf89",section:"Awards",handler:()=>{window.location.href="/awards/9_project/"}},{id:"projects-samsung-ai-challenge",title:"Samsung AI Challenge",description:"Competition on Developing Domain Adaptive Semantic Segmentation Algorithms for Autonomous Driving (1st place)2024.09",section:"Projects",handler:()=>{window.location.href="/projects/competetion/1_competetion_ssai/"}},{id:"projects-google-coral-board-challenge",title:"Google Coral board challenge",description:"Navible: Navigation for Blinds (1st place)2022.08",section:"Projects",handler:()=>{window.location.href="/projects/competetion/2_competetion_coral/"}},{id:"projects-crowd2crowd",title:"Crowd2Crowd",description:"C2C: Bidirectional data market platform for on-demand dataset request with automatic data quality control2023 SS",section:"Projects",handler:()=>{window.location.href="/projects/coursework/1_coursework_c2c/"}},{id:"projects-nutriseeon",title:"NutriSeeON",description:"NutriSeeON: Virtual eyes to shop healthy groceries2020 SS",section:"Projects",handler:()=>{window.location.href="/projects/coursework/2_coursework_nutriseeon/"}},{id:"projects-tamraatlas",title:"TamraAtlas",description:"TamrAtlas: Instagram Photo Atlas for Exploring the Trends in Jeju2020 FW",section:"Projects",handler:()=>{window.location.href="/projects/coursework/3_coursework_tamra-atlas/"}},{id:"projects-groomingdb",title:"GroomingDB",description:"Search Engine for sheltered Animals2022 SS",section:"Projects",handler:()=>{window.location.href="/projects/coursework/4_coursework_groomingDB/"}},{id:"projects-adaptive-sensing-for-dnns",title:"Adaptive Sensing for DNNs",description:"S4D: Providing proper glasses (sensor control) would be better than solely depending on over-training the brain (neural network)!2023.08 - current",section:"Projects",handler:()=>{window.location.href="/projects/research/0_research_s4d/"}},{id:"projects-mirror",title:"MIRROR",description:"MIRROR: Towards Generalizable On-Device Video Virtual Try-On for Mobile Shopping.2022.09 - 2024.06",section:"Projects",handler:()=>{window.location.href="/projects/research/1_research_mirror/"}},{id:"projects-mrviz",title:"MRViz",description:"MRViz: Visualization System for Analyzing Reliability of CNN-based Deep Learning Model2021.03 - 2022.02",section:"Projects",handler:()=>{window.location.href="/projects/research/2_research_mrviz/"}},{id:"projects-silverstone",title:"SilverStone",description:"SilverStone: Research and development of privacy-preserving techniques for deep learning technologies in server offloading scenarios.2019.05 - 2019.10",section:"Projects",handler:()=>{window.location.href="/projects/research/3_research_silverstone/"}},{id:"projects-colonood",title:"ColonOOD",description:"ColonOOD: A Complete Pipeline for Optical Diagnosis of Colorectal Polyps Integrating Out\u2011of\u2011Distribution Detection and Uncertainty Quantification2024.04 - 2025.06",section:"Projects",handler:()=>{window.location.href="/projects/research/4_research_gastro/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%62%65%73%68%75%39%34%30%37@%73%6E%75.%61%63.%6B%72","_blank")}},{id:"socials-google-scholar",title:"Google Scholar",section:"Socials",handler:()=>{window.open("https://scholar.google.com/citations?user=VihDCUsAAAAJ","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/edw2n","_blank")}},{id:"socials-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/eunsu-baek-4449282a9","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js"></script> </body> </html>