---
---

@string{aps = {American Physical Society,}}


@inproceedings{lens,
author = {Sehyun Park* and Dongheon Lee* and Ji Young Lee and Jaeyoung Chun and Ji Young Chang and Eunsu Baek and Eun Hyo Jin and Hyung‑Sin Kim†},
title = {ColonOOD: A Complete Pipeline for Optical Diagnosis of Colorectal Polyps Integrating Out‑of‑Distribution Detection and
Uncertainty Quantification},
booktitle = {The Thirteenth International Conference on Learning Representations (ICLR) 2025},
year = {2025},
month = {April},
abstract = {Domain shift remains a persistent challenge in deep-learning-based computer vision, often requiring extensive model modifications or large labeled datasets to address. Inspired by human visual perception, which adjusts input quality through corrective lenses rather than over-training the brain, we propose Lens, a novel camera sensor control method that enhances model performance by capturing high-quality images from the model's perspective rather than relying on traditional human-centric sensor control. Lens is lightweight and adapts sensor parameters to specific models and scenes in real-time. At its core, Lens utilizes VisiT, a training-free, model-specific quality indicator that evaluates individual unlabeled samples at test time using confidence scores without additional adaptation costs. To validate Lens, we introduce ImageNet-ES Diverse, a new benchmark dataset capturing natural perturbations from varying sensor and lighting conditions. Extensive experiments on both ImageNet-ES and our new ImageNet-ES Diverse show that Lens significantly improves model accuracy across various baseline schemes for sensor control and model modification while maintaining low latency in image captures. Lens effectively compensates for large model size differences and integrates synergistically with model improvement techniques. Our code and dataset are available at github.com/Edw2n/Lens.git.},
code={https://github.com/Edw2n/Lens},
pdf={https://openreview.net/pdf?id=He2FGdmsas},
bibtex_show={true},
preview={ImageNet-ES.jpg},
selected={true},
abbr={ICLR},
tags={s4d},
}

@inproceedings{lens,
author = {Eunsu Baek and Taesik Gong and Hyung‑Sin Kim},
title = {Adaptive Camera Sensor for Vision Model},
booktitle = {The Thirteenth International Conference on Learning Representations (ICLR) 2025},
year = {2025},
month = {April},
abstract = {Domain shift remains a persistent challenge in deep-learning-based computer vision, often requiring extensive model modifications or large labeled datasets to address. Inspired by human visual perception, which adjusts input quality through corrective lenses rather than over-training the brain, we propose Lens, a novel camera sensor control method that enhances model performance by capturing high-quality images from the model's perspective rather than relying on traditional human-centric sensor control. Lens is lightweight and adapts sensor parameters to specific models and scenes in real-time. At its core, Lens utilizes VisiT, a training-free, model-specific quality indicator that evaluates individual unlabeled samples at test time using confidence scores without additional adaptation costs. To validate Lens, we introduce ImageNet-ES Diverse, a new benchmark dataset capturing natural perturbations from varying sensor and lighting conditions. Extensive experiments on both ImageNet-ES and our new ImageNet-ES Diverse show that Lens significantly improves model accuracy across various baseline schemes for sensor control and model modification while maintaining low latency in image captures. Lens effectively compensates for large model size differences and integrates synergistically with model improvement techniques. Our code and dataset are available at github.com/Edw2n/Lens.git.},
code={https://github.com/Edw2n/Lens},
pdf={https://openreview.net/pdf?id=He2FGdmsas},
bibtex_show={true},
preview={ImageNet-ES.jpg},
selected={true},
abbr={ICLR},
tags={s4d},
}

@inproceedings{imagenet-es,
author = {Eunsu Baek and Keondo Park and Jiyoon Kim and Hyung-Sin Kim},
title = {Unexplored Faces of Robustness and Out-of-Distribution: Covariate Shifts in Environment and Sensor Domains},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
year = {2024},
month = {june},
abstract = {Computer vision applications predict on digital images acquired by a camera from physical scenes through light. However, conventional robustness benchmarks rely on perturbations in digitized images, diverging from distribution shifts occurring in the image acquisition process. To bridge this gap, we introduce a new distribution shift dataset, ImageNet-ES, comprising variations in environmental and camera sensor factors by directly capturing 202k images with a real camera in a controllable testbed. With the new dataset, we evaluate out-of-distribution (OOD) detection and model robustness. We find that existing OOD detection methods do not cope with the covariate shifts in ImageNet-ES, implying that the definition and detection of OOD should be revisited to embrace real-world distribution shifts. We also observe that the model becomes more robust in both ImageNet-C and -ES by learning environment and sensor variations in addition to existing digital augmentations. Lastly, our results suggest that effective shift mitigation via camera sensor control can significantly improve performance without increasing model size. With these findings, our benchmark may aid future research on robustness, OOD, and camera sensor control for computer vision. Our code and dataset are available at this https://github.com/Edw2n/ImageNet-ES},
code={https://github.com/Edw2n/ImageNet-ES},
video={https://youtu.be/rdfUkYSWpik?feature=shared},
pdf={https://arxiv.org/abs/2404.15882},
bibtex_show={true},
preview={ImageNet-ES.jpg},
selected={true},
abbr={CVPR},
tags={s4d},
}

@article{10.1145/3631420,
author = {Kang, Dong-Sig and Baek, Eunsu and Son, Sungwook and Lee, Youngki and Gong, Taesik and Kim, Hyung-Sin},
title = {[IMWUT 2023 / UbiComp 2024] MIRROR: Towards Generalizable On-Device Video Virtual Try-On for Mobile Shopping},
year = {2024},
issue_date = {December 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {4},
url = {https://doi.org/10.1145/3631420},
doi = {10.1145/3631420},
abstract = {We present MIRROR, an on-device video virtual try-on (VTO) system that provides realistic, private, and rapid experiences in mobile clothes shopping. Despite recent advancements in generative adversarial networks (GANs) for VTO, designing MIRROR involves two challenges: (1) data discrepancy due to restricted training data that miss various poses, body sizes, and backgrounds and (2) local computation overhead that uses up 24\% of battery for converting only a single video. To alleviate the problems, we propose a generalizable VTO GAN that not only discerns intricate human body semantics but also captures domain-invariant features without requiring additional training data. In addition, we craft lightweight, reliable clothes/pose-tracking that generates refined pixel-wise warping flow without neural-net computation. As a holistic system, MIRROR integrates the new VTO GAN and tracking method with meticulous pre/post-processing, operating in two distinct phases (on/offline). Our results on Android smartphones and real-world user videos show that compared to a cutting-edge VTO GAN, MIRROR achieves 6.5x better accuracy with 20.1x faster video conversion and 16.9x less energy consumption.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = {jan},
articleno = {163},
numpages = {27},
bibtex_show={true},
abbr={IMWUT/UbiComp},
video={https://youtu.be/wGtxRnmfNIw?feature=sharedhttps://youtu.be/wGtxRnmfNIw?feature=shared},
html={https://dl.acm.org/doi/abs/10.1145/3631420},
preview={MIRROR_Virtual_TryOn.gif},
selected={true},
pdf={https://dl.acm.org/doi/pdf/10.1145/3631420},
code={https://github.com/Ds-Kang/MIRROR.git},
tags={mirror},
}


@inproceedings{10.1145/3643832.3661842,
author = {Dongha Ahn and Dong-Sig Kang and Eunsu Baek and Hyung-Sin Kim},
title = {Demo: On-Device Video Virtual Try-On for Mobile Shopping},
year = {2024},
month = {June},
isbn = {9798400705816},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643832.3661842},
doi = {10.1145/3643832.3661842},
abstract = {Virtual try-on (VTO) superimposes clothing over user image or video, enhancing online shopping experience. On-device VTO can preserve user privacy but most VTO techniques cannot be run on resource-constrained devices due to excessive computation overhead. In this demo, we demonstrate a novel Android application for on-device video VTO referring to MIRROR, the state-of-the-art mobile VTO system. The application minimizes video generation time by splitting the process into two phases, achieving 0.76 minutes to convert 10-second-long video on Galaxy S24 Ultra. Our application was evaluated as 78.5 score (above average) in SUS usability test. A companion video is provided at: https://youtu.be/YTExc8W5BzM},
booktitle = {Proceedings of the 22nd Annual International Conference on Mobile Systems, Applications and Services},
pages = {610–611},
numpages = {2},
keywords = {virtual try-on, video, mobile system, on-device computing},
location = {<conf-loc> <city>Minato-ku, Tokyo</city>, <country>Japan</country> </conf-loc>},
series = {MOBISYS '24},
abbr={MobiSys},
html={https://dl.acm.org/doi/10.1145/3643832.3661842},
code={https://github.com/Ds-Kang/MIRROR.git},
video={https://youtu.be/YTExc8W5BzM},
pdf={https://dl.acm.org/doi/pdf/10.1145/3643832.3661842},
bibtex_show={true},
preview={Demo_OnDevice_Video_Virtual_TryOn_for_Mobile_Shopping.gif},
tags={mirror},
}
